import os
import time
from tqdm import tqdm
import pandas as pd
import numpy as np
from src.preprocessing import load_and_clean_data
from src.arima_baseline import run_arima
from src.regression_ols import run_ols, run_ols_feature_selection
from src.ml_models import run_rf, run_xgboost, run_mlp
from src.bayesian_dlm_mcmc import run_dlm_pymc
from src.bayesian_dlm_smc import run_dlm_numpyro
from src.metrics import plot_model_comparison
from src.bayesian_dlm_reg import run_dlm_dynamic_regression

# ğŸ“ Create folders if they don't exist
os.makedirs("results", exist_ok=True)

# âš™ï¸ Dataset and variables
data_path = "datasetcompleto.csv"
target_col = "CPI - YoY"

print("ğŸš€ Starting Inflation Forecasting Model Comparison")
print("=" * 60)

# ğŸ“Š 1. Preprocessing
print("ğŸ“Š Loading and preprocessing data...")
df = load_and_clean_data(data_path)
print(f"âœ… Data loaded: {len(df)} observations")
print()

# Define models to run
models = [
    ("ARIMA", "ğŸ“ˆ Classical Time Series"),
    ("OLS", "ğŸ“ˆ Linear Regression"),
    ("OLS + RFE", "ğŸ“ˆ Feature Selection"),
    ("Random Forest", "ğŸ¤– Ensemble Learning"),
    ("XGBoost", "ğŸ¤– Gradient Boosting"),
    ("MLP", "ğŸ¤– Neural Network"),
    ("Bayesian DLM (MCMC)", "ğŸ§  Bayesian MCMC"),
    ("Bayesian DLM (SMC)", "ğŸ§  Sequential Monte Carlo"),
    ("Bayesian DLM (DR)", "ğŸ§  Dynamic Coefficients")
]

# Initialize results dictionary
rmse_results = {}

# Create main progress bar
main_pbar = tqdm(models, desc="ğŸ”„ Running Models", unit="model", 
                 bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')

for model_name, model_desc in main_pbar:
    main_pbar.set_description(f"ğŸ”„ {model_desc}")
    
    try:
        start_time = time.time()
        
        if model_name == "ARIMA":
            _, rmse, best_order = run_arima(df, target_col)
            
        elif model_name == "OLS":
            _, rmse = run_ols(df, target_col)
            
        elif model_name == "OLS + RFE":
            _, _, rmse = run_ols_feature_selection(df, target_col)
            
        elif model_name == "Random Forest":
            _, rmse = run_rf(df, target_col)
            
        elif model_name == "XGBoost":
            _, rmse = run_xgboost(df, target_col)
            
        elif model_name == "MLP":
            _, rmse = run_mlp(df, target_col)
            
        elif model_name == "Bayesian DLM (MCMC)":
            _, rmse = run_dlm_pymc(df, target_col)
            
        elif model_name == "Bayesian DLM (SMC)":
            _, rmse = run_dlm_numpyro(df, target_col)
        elif model_name == "Bayesian DLM (DR)":
            trace, rmse = run_dlm_dynamic_regression(df, target_col)
                    
        # Store result
        rmse_results[model_name] = rmse
        
        # Calculate execution time
        exec_time = time.time() - start_time
        
        # Update progress bar with result
        main_pbar.set_postfix({
            'RMSE': f'{rmse:.4f}', 
            'Time': f'{exec_time:.1f}s'
        })
        
        print(f"\nâœ… {model_name}: RMSE = {rmse:.4f} (took {exec_time:.1f}s)")
        
    except Exception as e:
        print(f"\nâŒ Error running {model_name}: {str(e)}")
        rmse_results[model_name] = float('inf')  # Mark as failed
        continue

print("\n" + "=" * 60)
print("ğŸ“Š Generating comparison plots and saving results...")

# ğŸ“Š 5. Comparison
plot_model_comparison(rmse_results)

# ğŸ“ Save RMSE to CSV
results_df = pd.DataFrame.from_dict(rmse_results, orient='index', columns=['RMSE'])
results_df = results_df.sort_values('RMSE')  # Sort by best performance
results_df.to_csv("results/rmse_summary.csv")

print("\nğŸ† FINAL RESULTS RANKING:")
print("-" * 40)
for i, (model, rmse) in enumerate(results_df.iterrows(), 1):
    status = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i:2d}."
    print(f"{status} {model:<25} RMSE: {rmse['RMSE']:.4f}")

print(f"\nâœ… All models completed! Results saved in /results")
print("ğŸ“ Files generated:")
print("   â€¢ rmse_summary.csv - Detailed results")
print("   â€¢ model_comparison.png - Visual comparison")
print("   â€¢ Individual model plots in /results folder")

# Optional: Show execution summary
total_successful = sum(1 for rmse in rmse_results.values() if rmse != float('inf'))
print(f"\nğŸ“ˆ Execution Summary: {total_successful}/{len(models)} models completed successfully")